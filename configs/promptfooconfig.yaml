description: "Evaluation for Social Safety Net Chatbot (letter-only accuracy + rationale & citations)"

providers:
  - file: file://configs/providers.yaml

prompts:
  - id: file://prompts/mcq-letter-only.txt
    label: mcq-letter-only
  - id: file://prompts/mcq-rationale-citations.txt
    label: mcq-rationale-citations

defaultTest:
  vars:
    uniqueSessionId: file://./generateUniqueId.js
  options:
    temperature: 0
    timeout: 360000
  evaluateOptions:
    seed: 42
    maxConcurrency: 4
  view:
    showLatency: true
    showCosts: true

tests:
  - dataset: file://datasets/tests.csv
    prompt: file://prompts/mcq-letter-only.txt
    tags: ["test", "accuracy"]
    assertions:
      - type: equals
        value: "{{answer_key}}"
        trim: true
        ignoreCase: true
      - type: not-contains
        value: "I can't help with that"
      - type: latency
        max: 8000

  - dataset: file://datasets/tests.csv
    prompt: file://prompts/mcq-rationale-citations.txt
    tags: ["test", "rationale"]
    assertions:
      - type: not-contains
        value: "I can't help with that"
      - type: latency
        max: 12000
    graders:
      - rubric: rubrics/rationale_quality.md
        provider: openai:gpt-4o-mini
        weight: 0.7
      - rubric: rubrics/groundedness.md
        provider: openai:gpt-4o-mini
        weight: 0.3

scoring:
  - file: file://configs/scoring.yaml

view:
  showLatency: true
  groupBy: ["provider", "tags"]

outputPath: "results/${ISO_TIMESTAMP}"

providersOptions:
  http:nava-dst:
    headers:
      Content-Type: application/json